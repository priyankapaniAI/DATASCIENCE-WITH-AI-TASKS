{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1871d59-47f2-4780-8828-d6b6a07d6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\panip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\panip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP full/JJ stack/NN)\n",
      "  (NP datasceince/NN)\n",
      "  ,/,\n",
      "  (NP generative/JJ ai/NN)\n",
      "  ,/,\n",
      "  (NP agenti/NN)\n",
      "  (NP ai/NN)\n",
      "  ,/,\n",
      "  (NP llm/JJ model/NN)\n",
      "  (VP keep/VB (NP increase/NN))\n",
      "  (PP by/IN (NP different/JJ compnay/NN)))\n"
     ]
    }
   ],
   "source": [
    "# chunking in nlp\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "\n",
    "# Download necessary NLTK data files (only need to do this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"full stack datasceince, generative ai, agenti ai, llm model keep increase by different compnay\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform part-of-speech tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Define a chunk grammar\n",
    "chunk_grammar = r\"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN>}   # Noun Phrase\n",
    "  VP: {<VB.*><NP|PP>*}    # Verb Phrase\n",
    "  PP: {<IN><NP>}          # Prepositional Phrase\n",
    "\"\"\"\n",
    "\n",
    "# Create a chunk parser\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# Parse the tagged tokens\n",
    "chunked = chunk_parser.parse(tagged_tokens)\n",
    "\n",
    "# Print the chunked output\n",
    "print(chunked)\n",
    "\n",
    "# Optionally, you can visualize the chunks\n",
    "chunked.draw()\n",
    "#pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load a pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can replace with any other LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def chunk_text(text, max_length=512):\n",
    "    \"\"\"Chunk text into smaller pieces.\"\"\"\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "    chunks = []\n",
    "   \n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def generate_responses(chunks):\n",
    "    \"\"\"Generate responses for each chunk using the LLM.\"\"\"\n",
    "    responses = []\n",
    "    for chunk in chunks:\n",
    "        input_ids = chunk.unsqueeze(0)  # Add batch dimension\n",
    "        output = model.generate(input_ids, max_length=100)  # Generate response\n",
    "        responses.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "   \n",
    "    return responses\n",
    "\n",
    "# Example long text\n",
    "long_text = \"Your long text goes here. \" * 50  # Repeat to simulate long text\n",
    "\n",
    "# Chunk the text\n",
    "chunks = chunk_text(long_text)\n",
    "\n",
    "# Generate responses for each chunk\n",
    "responses = generate_responses(chunks)\n",
    "\n",
    "# Print the responses\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response for chunk {i+1}:\\n{response}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610dfab6-d46f-49b6-9f7f-23f4b4dceedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'natural': [ 7.0871073e-03 -1.5660768e-03  7.9488354e-03 -9.4871745e-03\n",
      " -8.0284970e-03 -6.6400990e-03 -4.0015476e-03  4.9885288e-03\n",
      " -3.8152416e-03 -8.3194422e-03  8.4130447e-03 -3.7484344e-03\n",
      "  8.6112693e-03 -4.8967898e-03  3.9208783e-03  4.9224664e-03\n",
      "  2.3913656e-03 -2.8193861e-03  2.8476808e-03 -8.2573323e-03\n",
      " -2.7654648e-03 -2.5892439e-03  7.2514298e-03 -3.4650220e-03\n",
      " -6.5993094e-03  4.3411572e-03 -4.7505071e-04 -3.5997534e-03\n",
      "  6.8830699e-03  3.8707533e-03 -3.8990439e-03  7.7067316e-04\n",
      "  9.1452012e-03  7.7572106e-03  6.3621844e-03  4.6685389e-03\n",
      "  2.3842277e-03 -1.8401309e-03 -6.3707866e-03 -3.0378028e-04\n",
      " -1.5659938e-03 -5.7153020e-04 -6.2632342e-03  7.4341921e-03\n",
      " -6.5920809e-03 -7.2400346e-03 -2.7571656e-03 -1.5145801e-03\n",
      " -7.6370556e-03  6.9744798e-04 -5.3264131e-03 -1.2755553e-03\n",
      " -7.3655494e-03  1.9596377e-03  3.2743122e-03 -2.3610632e-05\n",
      " -5.4481993e-03 -1.7260357e-03  7.0865974e-03  3.7349791e-03\n",
      " -8.8830115e-03 -3.4147073e-03  2.3562761e-03  2.1377914e-03\n",
      " -9.4665680e-03  4.5712972e-03 -8.6584333e-03 -7.3861703e-03\n",
      "  3.4812242e-03 -3.4705279e-03  3.5626595e-03  8.8948468e-03\n",
      " -3.5732167e-03  9.3225371e-03  1.7094795e-03  9.8469993e-03\n",
      "  5.7038716e-03 -9.1516990e-03 -3.3299716e-03  6.5300702e-03\n",
      "  5.6029037e-03  8.7044174e-03  6.9279517e-03  8.0395686e-03\n",
      " -9.8222801e-03  4.2993790e-03 -5.0301799e-03  3.5109802e-03\n",
      "  6.0565462e-03  4.3933080e-03  7.5116665e-03  1.4980425e-03\n",
      " -1.2643855e-03  5.7683820e-03 -5.6377216e-03  4.0403826e-05\n",
      "  9.4576264e-03 -5.4800059e-03  3.8157641e-03 -8.1146732e-03]\n",
      "Words similar to 'natural': [('I', 0.12812528014183044), ('love', 0.10942602902650833), ('Word2Vec', 0.10885821282863617), ('is', 0.06285625696182251), ('a', 0.05048368126153946)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for training\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"Word2Vec\", \"is\", \"a\", \"great\", \"tool\"],\n",
    "    [\"Machine\", \"learning\", \"is\", \"fun\"],\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Get the vector for a word\n",
    "vector = model.wv['natural']\n",
    "print(\"Vector for 'natural':\", vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('natural', topn=5)\n",
    "print(\"Words similar to 'natural':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9721fcef-7c60-4534-ae1c-288bf4be0028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Vector for 'language': [ 9.4794443e-05  3.0776660e-03 -6.8129268e-03 -1.3756783e-03\n",
      "  7.6698321e-03  7.3483307e-03 -3.6729362e-03  2.6408839e-03\n",
      " -8.3165076e-03  6.2072724e-03 -4.6391813e-03 -3.1636052e-03\n",
      "  9.3106655e-03  8.7376230e-04  7.4904198e-03 -6.0752141e-03\n",
      "  5.1592872e-03  9.9243205e-03 -8.4574828e-03 -5.1340456e-03\n",
      " -7.0650815e-03 -4.8629697e-03 -3.7796097e-03 -8.5361497e-03\n",
      "  7.9556443e-03 -4.8439130e-03  8.4241610e-03  5.2615325e-03\n",
      " -6.5502375e-03  3.9581223e-03  5.4700365e-03 -7.4268035e-03\n",
      " -7.4072029e-03 -2.4764745e-03 -8.6256117e-03 -1.5829162e-03\n",
      " -4.0474746e-04  3.3000517e-03  1.4428297e-03 -8.8208629e-04\n",
      " -5.5940356e-03  1.7293066e-03 -8.9629035e-04  6.7937491e-03\n",
      "  3.9739395e-03  4.5298305e-03  1.4351519e-03 -2.7006667e-03\n",
      " -4.3665408e-03 -1.0332628e-03  1.4375091e-03 -2.6469158e-03\n",
      " -7.0722066e-03 -7.8058685e-03 -9.1226082e-03 -5.9341355e-03\n",
      " -1.8468037e-03 -4.3235817e-03 -6.4619821e-03 -3.7178723e-03\n",
      "  4.2904112e-03 -3.7397402e-03  8.3768284e-03  1.5343785e-03\n",
      " -7.2409823e-03  9.4339680e-03  7.6326625e-03  5.4943082e-03\n",
      " -6.8490817e-03  5.8238246e-03  4.0079155e-03  5.1836823e-03\n",
      "  4.2568049e-03  1.9397212e-03 -3.1705969e-03  8.3537176e-03\n",
      "  9.6112443e-03  3.7936033e-03 -2.8369424e-03  6.7305832e-06\n",
      "  1.2181988e-03 -8.4593873e-03 -8.2249697e-03 -2.3308117e-04\n",
      "  1.2385092e-03 -5.7431920e-03 -4.7247363e-03 -7.3465765e-03\n",
      "  8.3276192e-03  1.2043064e-04 -4.5089805e-03  5.7007410e-03\n",
      "  9.1796070e-03 -4.1010864e-03  7.9633193e-03  5.3759255e-03\n",
      "  5.8792117e-03  5.1329390e-04  8.2118409e-03 -7.0186048e-03]\n",
      "Skip-gram Vector for 'language': [ 9.42478000e-05  3.07589723e-03 -6.81467308e-03 -1.37446506e-03\n",
      "  7.66968913e-03  7.34756188e-03 -3.67422565e-03  2.64107878e-03\n",
      " -8.31711013e-03  6.20709499e-03 -4.63969819e-03 -3.16430302e-03\n",
      "  9.31042060e-03  8.73924117e-04  7.48968776e-03 -6.07334916e-03\n",
      "  5.15946327e-03  9.92259663e-03 -8.45542643e-03 -5.13521628e-03\n",
      " -7.06540514e-03 -4.86227963e-03 -3.78140691e-03 -8.53707641e-03\n",
      "  7.95734860e-03 -4.84467065e-03  8.42242502e-03  5.26282424e-03\n",
      " -6.55034464e-03  3.95740150e-03  5.47204912e-03 -7.42573338e-03\n",
      " -7.40648946e-03 -2.47416925e-03 -8.62730667e-03 -1.58222113e-03\n",
      " -4.05228348e-04  3.30146588e-03  1.44360668e-03 -8.81455548e-04\n",
      " -5.59283607e-03  1.72997033e-03 -8.96777317e-04  6.79324893e-03\n",
      "  3.97348078e-03  4.53017978e-03  1.43477262e-03 -2.69988249e-03\n",
      " -4.36586002e-03 -1.03212311e-03  1.43821072e-03 -2.64558522e-03\n",
      " -7.07180146e-03 -7.80386291e-03 -9.12202150e-03 -5.93545521e-03\n",
      " -1.84791500e-03 -4.32417588e-03 -6.46110671e-03 -3.71831306e-03\n",
      "  4.28991020e-03 -3.73849226e-03  8.37847777e-03  1.53453019e-03\n",
      " -7.24213943e-03  9.43322573e-03  7.63178896e-03  5.49173821e-03\n",
      " -6.84854668e-03  5.82335703e-03  4.00784146e-03  5.18454844e-03\n",
      "  4.25614510e-03  1.93787215e-03 -3.17041040e-03  8.35264847e-03\n",
      "  9.61161498e-03  3.79254599e-03 -2.83515733e-03  7.22470668e-06\n",
      "  1.21920742e-03 -8.46034940e-03 -8.22578371e-03 -2.32077524e-04\n",
      "  1.23913167e-03 -5.74427750e-03 -4.72703110e-03 -7.34633533e-03\n",
      "  8.32915492e-03  1.21630226e-04 -4.51042084e-03  5.70245273e-03\n",
      "  9.18024499e-03 -4.09950921e-03  7.96421152e-03  5.37427003e-03\n",
      "  5.87765872e-03  5.15150023e-04  8.21374636e-03 -7.01800501e-03]\n",
      "CBOW - Words similar to 'language': [('tool', 0.1991048902273178), ('Word2Vec', 0.17271503806114197), ('Natural', 0.170233353972435), ('learning', 0.14595259726047516), ('fun', 0.06409329921007156)]\n",
      "Skip-gram - Words similar to 'language': [('tool', 0.19910898804664612), ('Word2Vec', 0.17269527912139893), ('Natural', 0.17020359635353088), ('learning', 0.1459759771823883), ('fun', 0.06406981498003006)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences for training\n",
    "sentences = [\n",
    "    [\"I\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"Word2Vec\", \"is\", \"a\", \"great\", \"tool\"],\n",
    "    [\"Machine\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"Natural\", \"language\", \"processing\", \"is\", \"awesome\"]\n",
    "]\n",
    "\n",
    "# CBOW Model\n",
    "cbow_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Skip-gram Model\n",
    "skipgram_model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Example: Getting the vector for a word\n",
    "word = \"language\"\n",
    "cbow_vector = cbow_model.wv[word]\n",
    "skipgram_vector = skipgram_model.wv[word]\n",
    "\n",
    "print(f\"CBOW Vector for '{word}':\", cbow_vector)\n",
    "print(f\"Skip-gram Vector for '{word}':\", skipgram_vector)\n",
    "\n",
    "# Example: Finding similar words\n",
    "cbow_similar_words = cbow_model.wv.most_similar(word, topn=5)\n",
    "skipgram_similar_words = skipgram_model.wv.most_similar(word, topn=5)\n",
    "\n",
    "print(f\"CBOW - Words similar to '{word}':\", cbow_similar_words)\n",
    "print(f\"Skip-gram - Words similar to '{word}':\", skipgram_similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c6a9d-ce77-4144-9477-8b08107d3515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
